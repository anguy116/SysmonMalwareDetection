from pyspark.sql import SparkSession
from pyspark.sql.window import Window
import pyspark.sql.functions as fn
import sys
import os

def sourcedDf(df):
    cols = ["@timestamp", "process.pid", "process.executable"]

    # think of bringing in type
    evtcols = ["code", "kind", "action", "type"]
    res = (df
           .select(
        ["_source"] + list(map(lambda x: fn.col("_source.event." + x), evtcols)) +
        list(map(lambda x: fn.col("_source." + x), cols)))
           .withColumn("message_length", fn.length("_source.message"))
           .withColumn("parentpid_present", fn.when(fn.col("_source.process.parent.pid").isNotNull(), 1).otherwise(0))
           .withColumn("parentexe_present", fn.when(fn.col("_source.process.parent.executable").isNotNull(), 1).otherwise(0))
           .withColumn("eventid", fn.monotonically_increasing_id() + 1)
           .withColumn('timestamp', fn.col("@timestamp").cast('timestamp'))
           # .withColumn('type',fn.explode(fn.col("type")))
           .drop("@timestamp")
           .orderBy("timestamp")
           )
    #     res=res.withColumn("type",fn.explode(res.type))
    return res


def rollingWindowCount(df, cde, lag):
    w = (Window
         .orderBy("timestamp")
         .rowsBetween(-(lag), 0)
         )
    res = (df
           .withColumn(str(cde) + "orNot", fn.when(df.code == cde, 1).otherwise(0))
           .withColumn(str(cde) + "_eventwindow",
                       fn.sum(str(cde) + "orNot")
                       .over(w))
           .drop(str(cde) + "orNot"))

    return res


# lag in minutes
def rollingWindowTime(df, cde,time):
    tmp = df.withColumn("timeinmilli",
                        (fn.to_timestamp(fn.col("_source.@timestamp")).cast("double") * 1000).cast('long'))

    #               ???  * sec
    lag = lambda i: 1000 * 1 * i
    w = (Window
         .orderBy(fn.asc("timeinmilli"))
         .rangeBetween(-lag(time), 0))

    res = tmp \
        .withColumn("code_flag", fn.when(df.code == cde, 1).otherwise(0)) \
        .withColumn("eventid_"+str(cde) + "_timewindowcount", fn.sum(fn.col("code_flag")).over(w)) \
        .drop("timeinmilli","code_flag")

    return res


def previousTimeCreation(df):
    if df.filter(fn.col("_source.event.code") == 2).count() == 0:
        return df.withColumn("timebetweenfilechange_eventid_2",fn.lit(-1.00))

    res = (df
           .withColumn('newcreationtime', fn.when(df._source.event.code == 2, fn.to_timestamp(fn.col('_source.winlog.event_data.CreationUtcTime')).cast('double')).otherwise(fn.lit(0.00)))
           .withColumn('previouscreationtime', fn.when(df._source.event.code == 2, fn.to_timestamp(
        fn.col('_source.winlog.event_data.PreviousCreationUtcTime')).cast('double')).otherwise(fn.lit(0.00)))
           )

    fin = (res.withColumn("timebetweenfilechange_eventid_2",
                         (res.previouscreationtime * 1000).cast('long') - (res.newcreationtime * 1000).cast('long'))
           .drop('newcreationtime', 'previouscreationtime'))
    return fin


def badCount(df):
    words = ["fail", "tried", "invalid"]

    tmp = (df
           .withColumn("message", fn.col("_source.message"))
           .withColumn("failPresent", fn.when(fn.lower(fn.col("_source.message")).contains(words[0]), 1).otherwise(0))
           .withColumn("triedPresent", fn.when(fn.lower(fn.col("_source.message")).contains(words[1]), 1).otherwise(0))
           .withColumn("invalidPresent",
                       fn.when(fn.lower(fn.col("_source.message")).contains(words[2]), 1).otherwise(0))
           )
    return tmp


def processdf(df):
    source = df.transform(sourcedDf)
    sysmon_range = [i for i in range(1,26)]
    res = (source
            # doing sequential id counts
           .transform(lambda df: rollingWindowCount(df, 1, 100))
           .transform(lambda df: rollingWindowCount(df, 2, 100))
           .transform(lambda df: rollingWindowCount(df, 3, 100))
           .transform(lambda df: rollingWindowCount(df, 4, 100))
           .transform(lambda df: rollingWindowCount(df, 5, 100))
           .transform(lambda df: rollingWindowCount(df, 6, 100))
           .transform(lambda df: rollingWindowCount(df, 7, 100))
           .transform(lambda df: rollingWindowCount(df, 8, 100))
           .transform(lambda df: rollingWindowCount(df, 9, 100))
           .transform(lambda df: rollingWindowCount(df, 10, 100))
           .transform(lambda df: rollingWindowCount(df, 11, 100))
           .transform(lambda df: rollingWindowCount(df, 12, 100))
           .transform(lambda df: rollingWindowCount(df, 13, 100))
           .transform(lambda df: rollingWindowCount(df, 14, 100))
           .transform(lambda df: rollingWindowCount(df, 15, 100))
           .transform(lambda df: rollingWindowCount(df, 16, 100))
           .transform(lambda df: rollingWindowCount(df, 17, 100))
           .transform(lambda df: rollingWindowCount(df, 18, 100))
           .transform(lambda df: rollingWindowCount(df, 19, 100))
           .transform(lambda df: rollingWindowCount(df, 20, 100))
           .transform(lambda df: rollingWindowCount(df, 21, 100))
           .transform(lambda df: rollingWindowCount(df, 22, 100))
           .transform(lambda df: rollingWindowCount(df, 23, 100))
           .transform(lambda df: rollingWindowCount(df, 24, 100))
           .transform(lambda df: rollingWindowCount(df, 25, 100))
           .transform(lambda df: rollingWindowCount(df, 26, 100))

            # presence of bad key-words
           .transform(lambda df: badCount(df))

            # time-window peak back
           .transform(lambda df: rollingWindowTime(df, 13, 30))
           .transform(lambda df: rollingWindowTime(df, 11, 30))
           .transform(lambda df: rollingWindowTime(df, 6, 30))
           .transform(lambda df: rollingWindowTime(df, 7, 30))
           .transform(lambda df: rollingWindowTime(df, 3, 30))
           .transform(lambda df: rollingWindowTime(df, 25, 30))
           .transform(lambda df: rollingWindowTime(df, 16, 30))
           .transform(lambda df: rollingWindowTime(df, 17, 30))
           .transform(lambda df: rollingWindowTime(df, 23, 30))
           .transform(lambda df: rollingWindowTime(df, 22, 30))
           .transform(lambda df: rollingWindowTime(df, 15, 30))
           .transform(lambda df: rollingWindowTime(df, 10, 30))

           # if file creation time change, record time between changes
           .transform(lambda df: previousTimeCreation(df))
           .filter(fn.col("code").cast("int").isin(sysmon_range))
           .drop("_source","message","kind","action","executable","timestamp","code","type","pid","eventid")
           )

    return res

if __name__ == "__main__":
    if sys.argv[2] is not None:

        # folder directory where all files will be picked up
        coll_filePath = sys.argv[1]

        # folder path for dropping everything
        drop_filePath = sys.argv[2]

        # check if its just a file name
        if len(coll_filePath.split("."))>1:
            spark = SparkSession.builder.appName("csi4900_featureeng").getOrCreate()
            spark.conf.set('spark.sql.caseSensitive', True)
            spark.conf.set('spark.sql.caseSensitive', True)
            df = spark.read.format("json").option("header", True).load(coll_filePath)
            res = processdf(df)
            res.write.format("json").save(drop_filePath)

        for file in os.listdir(coll_filePath):
            spark = SparkSession.builder.appName("csi4900_featureeng").getOrCreate()
            spark.conf.set('spark.sql.caseSensitive', True)
            df = spark.read.format("json").option("header", True).load("/".join((coll_filePath,file)))
            res = processdf(df)
            res.write.format("json").save("/".join((drop_filePath,file.split(".")[0])))

