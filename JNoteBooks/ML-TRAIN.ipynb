{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c794ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b8ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting a confusion matrix for test results\n",
    "# obtained this from Marco Cerliani git@cerlymarco\n",
    "# https://github.com/cerlymarco/MEDIUM_NoteBook\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 14)\n",
    "\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfccd608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(\"OC-training\").getOrCreate()\n",
    "spark.conf.set('spark.sql.caseSensitive', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f836ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    path - abs path to root directory containing all sysmon log files collected by winlogbeats\n",
    "    \n",
    "    takes all json files and combines them into one homogenous pandas dataframe\n",
    "'''\n",
    "def prepareData(path):\n",
    "    df_list= []\n",
    "    \n",
    "    # looks for all subdirectories\n",
    "    for root,subdir,files in os.walk(path):\n",
    "        if len(files)>0:\n",
    "            for file in files:\n",
    "                if len(file.split(\".\"))>1 and file.split('.')[-1] == \"json\":\n",
    "                    filePath = \"/\".join((root,file))\n",
    "                    df = spark.read.format(\"json\").option(\"inferSchema\",True).load(filePath).drop(\"message_length\").toPandas()\n",
    "                    df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "'''\n",
    "    data - -pandas dataframe\n",
    "    splits the data into length 10 time series\n",
    "'''\n",
    "def createTimeSeries(data):\n",
    "    batch = []\n",
    "    for j in range(10,len(data)-1,10):\n",
    "        df = data[j-10:j].values\n",
    "        if(not math.isnan(df.sum())):\n",
    "            batch.append(df)\n",
    "    return batch\n",
    "\n",
    "'''\n",
    "    filePath - abs file path to benign json data\n",
    "    \n",
    "    splits data the same as createTimeSeries function\n",
    "'''\n",
    "def datagenBenign(filePath):\n",
    "    # read benign data\n",
    "    batch = []\n",
    "    data = spark.read.format(\"json\").option(\"inferSchema\",True).load(filePath).drop(\"message_length\").toPandas()\n",
    "    normalized_df=((data-data.mean())/data.std()).fillna(0.0)\n",
    "    for j in range(10,len(data)-1,10):\n",
    "        batch.append(normalized_df[j-10:j].values)\n",
    "        \n",
    "    return np.asanyarray(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b12aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for splitting\n",
    "seed = 44\n",
    "# Process all malware samples\n",
    "bigDF =prepareData(\"C:/Users/aweso/Downloads/ALL_important/\")\n",
    "dataAsTimeSeries = np.array(createTimeSeries(bigDF))\n",
    "\n",
    "# split into train and hold out dataset\n",
    "train_X,test_X_mal = train_test_split(dataAsTimeSeries, test_size=0.2, random_state=seed)\n",
    "# reshapes the data into two identically shapped sub matrices, one for the malicious data and one for the pseudo benign created by the model\n",
    "train_y = np.array([np.array([np.repeat(1.0,10).reshape((-1,1)),np.repeat(0.0,10).reshape((-1,1))]) for i in range(0,train_X.shape[0])])\n",
    "\n",
    "# configuring test data\n",
    "test_mal = resample(test_X_mal,n_samples=185, random_state=seed)\n",
    "mal_test_labels = np.array([np.repeat(1.0,10).reshape((-1,1)) for i in range(0,test_mal.shape[0])])\n",
    "\n",
    "test_benign = datagenBenign(\"C:/Users/aweso/Downloads/ben_lz_all_important/benign3/part-00000-5d4fa1ac-c21e-4ccb-a04f-4d173e78d260-c000.json\")\n",
    "benign_test_labels = np.array([np.repeat(0.0,10).reshape((-1,1)) for i in range(0,test_benign.shape[0])])\n",
    "\n",
    "# Concatenate the data for testing\n",
    "test_data = np.concatenate([test_mal,test_benign])\n",
    "test_labels = np.concatenate([mal_test_labels,benign_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a3157b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 16)]     0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 10, 16)       0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " gaussian_noise (GaussianNoise)  (None, 10, 16)      0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 10, 16)       0           ['input_1[0][0]',                \n",
      "                                                                  'gaussian_noise[0][0]']         \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 10, 16)       0           ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10, 10)       170         ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10, 10)       110         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10, 1)        11          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 291\n",
      "Trainable params: 291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SHAPE = (train_X.shape[1],train_X.shape[2])\n",
    "\n",
    "def get_model(train=True):\n",
    "    \n",
    "    # standard input layer\n",
    "    initTensor = Input(SHAPE)\n",
    "    \n",
    "    # create gaussian noise layer\n",
    "    noise = Lambda(tf.zeros_like)(initTensor)\n",
    "    noise = GaussianNoise(0.01)(noise)\n",
    "    \n",
    "    # if in train mode will concatenate noise to the model\n",
    "    if train:\n",
    "        x = Lambda(lambda z: tf.concat(z,axis=0))([initTensor,noise])\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "    # else the classifier network will be separated\n",
    "    else:\n",
    "        x= initTensor\n",
    "    \n",
    "    # dense layers\n",
    "    x = Dense(10,activation='relu')(x)\n",
    "    x = Dense(10,activation='relu')(x)\n",
    "    \n",
    "    # prediction\n",
    "    out = Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    mdl = Model(initTensor,out)\n",
    "    return mdl\n",
    "    \n",
    "model = get_model(True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4580e3f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 177104848.0000\n",
      "Seen so far: 1 samples\n",
      "Training loss (for one batch) at step 200: 0.3467\n",
      "Seen so far: 201 samples\n",
      "Training loss (for one batch) at step 400: 0.3440\n",
      "Seen so far: 401 samples\n",
      "Training loss (for one batch) at step 600: 0.3420\n",
      "Seen so far: 601 samples\n",
      "Training loss (for one batch) at step 800: 0.3414\n",
      "Seen so far: 801 samples\n",
      "Training loss (for one batch) at step 1000: 0.3387\n",
      "Seen so far: 1001 samples\n",
      "Training loss (for one batch) at step 1200: 0.3370\n",
      "Seen so far: 1201 samples\n",
      "Training loss (for one batch) at step 1400: 0.3356\n",
      "Seen so far: 1401 samples\n",
      "Training loss (for one batch) at step 1600: 0.3342\n",
      "Seen so far: 1601 samples\n",
      "Training loss (for one batch) at step 1800: 0.3321\n",
      "Seen so far: 1801 samples\n",
      "Training loss (for one batch) at step 2000: 0.3306\n",
      "Seen so far: 2001 samples\n",
      "Training loss (for one batch) at step 2200: 0.3289\n",
      "Seen so far: 2201 samples\n",
      "Training loss (for one batch) at step 2400: 0.3275\n",
      "Seen so far: 2401 samples\n",
      "Training loss (for one batch) at step 2600: 0.3260\n",
      "Seen so far: 2601 samples\n",
      "Training loss (for one batch) at step 2800: 0.3243\n",
      "Seen so far: 2801 samples\n",
      "Training loss (for one batch) at step 3000: 0.3230\n",
      "Seen so far: 3001 samples\n",
      "Training loss (for one batch) at step 3200: 0.3211\n",
      "Seen so far: 3201 samples\n",
      "Training loss (for one batch) at step 3400: 0.3196\n",
      "Seen so far: 3401 samples\n",
      "Training loss (for one batch) at step 3600: 0.3204\n",
      "Seen so far: 3601 samples\n",
      "Training loss (for one batch) at step 3800: 0.3168\n",
      "Seen so far: 3801 samples\n",
      "Training loss (for one batch) at step 4000: 0.3147\n",
      "Seen so far: 4001 samples\n",
      "Training loss (for one batch) at step 4200: 0.3133\n",
      "Seen so far: 4201 samples\n",
      "Training loss (for one batch) at step 4400: 0.3118\n",
      "Seen so far: 4401 samples\n",
      "Training loss (for one batch) at step 4600: 0.3108\n",
      "Seen so far: 4601 samples\n",
      "Training loss (for one batch) at step 4800: 0.3089\n",
      "Seen so far: 4801 samples\n",
      "Training loss (for one batch) at step 5000: 0.3071\n",
      "Seen so far: 5001 samples\n",
      "Training loss (for one batch) at step 5200: 0.3057\n",
      "Seen so far: 5201 samples\n",
      "Training loss (for one batch) at step 5400: 0.3041\n",
      "Seen so far: 5401 samples\n",
      "Training loss (for one batch) at step 5600: 0.3027\n",
      "Seen so far: 5601 samples\n",
      "Training loss (for one batch) at step 5800: 0.3009\n",
      "Seen so far: 5801 samples\n",
      "Training loss (for one batch) at step 6000: 0.2993\n",
      "Seen so far: 6001 samples\n",
      "Training loss (for one batch) at step 6200: 0.2977\n",
      "Seen so far: 6201 samples\n",
      "Training loss (for one batch) at step 6400: 0.2960\n",
      "Seen so far: 6401 samples\n",
      "Training loss (for one batch) at step 6600: 0.2951\n",
      "Seen so far: 6601 samples\n",
      "Training loss (for one batch) at step 6800: 0.2929\n",
      "Seen so far: 6801 samples\n",
      "Training loss (for one batch) at step 7000: 0.2914\n",
      "Seen so far: 7001 samples\n",
      "Training loss (for one batch) at step 7200: 0.2901\n",
      "Seen so far: 7201 samples\n",
      "Training loss (for one batch) at step 7400: 0.2881\n",
      "Seen so far: 7401 samples\n",
      "Training loss (for one batch) at step 7600: 0.2867\n",
      "Seen so far: 7601 samples\n",
      "Training loss (for one batch) at step 7800: 0.2855\n",
      "Seen so far: 7801 samples\n",
      "Training loss (for one batch) at step 8000: 0.2839\n",
      "Seen so far: 8001 samples\n",
      "Training loss (for one batch) at step 8200: 0.2823\n",
      "Seen so far: 8201 samples\n",
      "Training loss (for one batch) at step 8400: 0.2806\n",
      "Seen so far: 8401 samples\n",
      "Training loss (for one batch) at step 8600: 0.2792\n",
      "Seen so far: 8601 samples\n",
      "Training loss (for one batch) at step 8800: 0.2774\n",
      "Seen so far: 8801 samples\n",
      "Training loss (for one batch) at step 9000: 0.2760\n",
      "Seen so far: 9001 samples\n",
      "Training loss (for one batch) at step 9200: 0.2745\n",
      "Seen so far: 9201 samples\n",
      "Training loss (for one batch) at step 9400: 0.2729\n",
      "Seen so far: 9401 samples\n",
      "Training loss (for one batch) at step 9600: 0.2713\n",
      "Seen so far: 9601 samples\n",
      "Training loss (for one batch) at step 9800: 0.2700\n",
      "Seen so far: 9801 samples\n",
      "Training loss (for one batch) at step 10000: 140174944.0000\n",
      "Seen so far: 10001 samples\n",
      "Training loss (for one batch) at step 10200: 0.2666\n",
      "Seen so far: 10201 samples\n",
      "Training loss (for one batch) at step 10400: 0.2652\n",
      "Seen so far: 10401 samples\n",
      "Training loss (for one batch) at step 10600: 137813296.0000\n",
      "Seen so far: 10601 samples\n",
      "Training loss (for one batch) at step 10800: 0.2619\n",
      "Seen so far: 10801 samples\n",
      "Training loss (for one batch) at step 11000: 0.2602\n",
      "Seen so far: 11001 samples\n",
      "Training loss (for one batch) at step 11200: 0.2589\n",
      "Seen so far: 11201 samples\n",
      "Training loss (for one batch) at step 11400: 135533184.0000\n",
      "Seen so far: 11401 samples\n",
      "Training loss (for one batch) at step 11600: 0.3247\n",
      "Seen so far: 11601 samples\n",
      "Training loss (for one batch) at step 11800: 0.2541\n",
      "Seen so far: 11801 samples\n",
      "Training loss (for one batch) at step 12000: 0.3397\n",
      "Seen so far: 12001 samples\n",
      "Training loss (for one batch) at step 12200: 0.2510\n",
      "Seen so far: 12201 samples\n",
      "Training loss (for one batch) at step 12400: 131818776.0000\n",
      "Seen so far: 12401 samples\n",
      "Training loss (for one batch) at step 12600: 0.2478\n",
      "Seen so far: 12601 samples\n",
      "Training loss (for one batch) at step 12800: 0.2462\n",
      "Seen so far: 12801 samples\n",
      "Training loss (for one batch) at step 13000: 0.2448\n",
      "Seen so far: 13001 samples\n",
      "Training loss (for one batch) at step 13200: 0.2434\n",
      "Seen so far: 13201 samples\n",
      "Training loss (for one batch) at step 13400: 0.2416\n",
      "Seen so far: 13401 samples\n",
      "Training loss (for one batch) at step 13600: 0.2399\n",
      "Seen so far: 13601 samples\n",
      "Training loss (for one batch) at step 13800: 0.2386\n",
      "Seen so far: 13801 samples\n",
      "Training loss (for one batch) at step 14000: 0.2371\n",
      "Seen so far: 14001 samples\n",
      "Training loss (for one batch) at step 14200: 0.2357\n",
      "Seen so far: 14201 samples\n",
      "Training loss (for one batch) at step 14400: 0.2340\n",
      "Seen so far: 14401 samples\n",
      "Training loss (for one batch) at step 14600: 0.2325\n",
      "Seen so far: 14601 samples\n",
      "Training loss (for one batch) at step 14800: 0.7284\n",
      "Seen so far: 14801 samples\n",
      "Training loss (for one batch) at step 15000: 0.2290\n",
      "Seen so far: 15001 samples\n",
      "Training loss (for one batch) at step 15200: 0.2275\n",
      "Seen so far: 15201 samples\n",
      "Training loss (for one batch) at step 15400: 2579065600.0000\n",
      "Seen so far: 15401 samples\n",
      "Training loss (for one batch) at step 15600: 0.2248\n",
      "Seen so far: 15601 samples\n",
      "Training loss (for one batch) at step 15800: 0.2236\n",
      "Seen so far: 15801 samples\n",
      "Training loss (for one batch) at step 16000: 0.2215\n",
      "Seen so far: 16001 samples\n",
      "Training loss (for one batch) at step 16200: 0.2203\n",
      "Seen so far: 16201 samples\n",
      "Training loss (for one batch) at step 16400: 0.2187\n",
      "Seen so far: 16401 samples\n",
      "Training loss (for one batch) at step 16600: 0.2172\n",
      "Seen so far: 16601 samples\n",
      "Training loss (for one batch) at step 16800: 0.5117\n",
      "Seen so far: 16801 samples\n",
      "Training loss (for one batch) at step 17000: 0.2144\n",
      "Seen so far: 17001 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 121043728.0000\n",
      "Seen so far: 1 samples\n",
      "Training loss (for one batch) at step 200: 0.2110\n",
      "Seen so far: 201 samples\n",
      "Training loss (for one batch) at step 400: 0.2097\n",
      "Seen so far: 401 samples\n",
      "Training loss (for one batch) at step 600: 0.2080\n",
      "Seen so far: 601 samples\n",
      "Training loss (for one batch) at step 800: 0.2064\n",
      "Seen so far: 801 samples\n",
      "Training loss (for one batch) at step 1000: 0.2050\n",
      "Seen so far: 1001 samples\n",
      "Training loss (for one batch) at step 1200: 0.2035\n",
      "Seen so far: 1201 samples\n",
      "Training loss (for one batch) at step 1400: 0.2021\n",
      "Seen so far: 1401 samples\n",
      "Training loss (for one batch) at step 1600: 0.2005\n",
      "Seen so far: 1601 samples\n",
      "Training loss (for one batch) at step 1800: 0.1993\n",
      "Seen so far: 1801 samples\n",
      "Training loss (for one batch) at step 2000: 0.1976\n",
      "Seen so far: 2001 samples\n",
      "Training loss (for one batch) at step 2200: 0.1961\n",
      "Seen so far: 2201 samples\n",
      "Training loss (for one batch) at step 2400: 0.1943\n",
      "Seen so far: 2401 samples\n",
      "Training loss (for one batch) at step 2600: 0.1932\n",
      "Seen so far: 2601 samples\n",
      "Training loss (for one batch) at step 2800: 0.1917\n",
      "Seen so far: 2801 samples\n",
      "Training loss (for one batch) at step 3000: 0.1899\n",
      "Seen so far: 3001 samples\n",
      "Training loss (for one batch) at step 3200: 0.1886\n",
      "Seen so far: 3201 samples\n",
      "Training loss (for one batch) at step 3400: 0.1871\n",
      "Seen so far: 3401 samples\n",
      "Training loss (for one batch) at step 3600: 0.1855\n",
      "Seen so far: 3601 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 3800: 0.1840\n",
      "Seen so far: 3801 samples\n",
      "Training loss (for one batch) at step 4000: 0.1826\n",
      "Seen so far: 4001 samples\n",
      "Training loss (for one batch) at step 4200: 0.1813\n",
      "Seen so far: 4201 samples\n",
      "Training loss (for one batch) at step 4400: 0.1800\n",
      "Seen so far: 4401 samples\n",
      "Training loss (for one batch) at step 4600: 0.1789\n",
      "Seen so far: 4601 samples\n",
      "Training loss (for one batch) at step 4800: 0.1768\n",
      "Seen so far: 4801 samples\n",
      "Training loss (for one batch) at step 5000: 0.1759\n",
      "Seen so far: 5001 samples\n",
      "Training loss (for one batch) at step 5200: 0.1744\n",
      "Seen so far: 5201 samples\n",
      "Training loss (for one batch) at step 5400: 0.1729\n",
      "Seen so far: 5401 samples\n",
      "Training loss (for one batch) at step 5600: 0.1712\n",
      "Seen so far: 5601 samples\n",
      "Training loss (for one batch) at step 5800: 0.1700\n",
      "Seen so far: 5801 samples\n",
      "Training loss (for one batch) at step 6000: 0.1685\n",
      "Seen so far: 6001 samples\n",
      "Training loss (for one batch) at step 6200: 0.1671\n",
      "Seen so far: 6201 samples\n",
      "Training loss (for one batch) at step 6400: 0.1655\n",
      "Seen so far: 6401 samples\n",
      "Training loss (for one batch) at step 6600: 0.1644\n",
      "Seen so far: 6601 samples\n",
      "Training loss (for one batch) at step 6800: 0.1627\n",
      "Seen so far: 6801 samples\n",
      "Training loss (for one batch) at step 7000: 0.1616\n",
      "Seen so far: 7001 samples\n",
      "Training loss (for one batch) at step 7200: 0.1600\n",
      "Seen so far: 7201 samples\n",
      "Training loss (for one batch) at step 7400: 0.1588\n",
      "Seen so far: 7401 samples\n",
      "Training loss (for one batch) at step 7600: 0.1576\n",
      "Seen so far: 7601 samples\n",
      "Training loss (for one batch) at step 7800: 0.1560\n",
      "Seen so far: 7801 samples\n",
      "Training loss (for one batch) at step 8000: 0.1550\n",
      "Seen so far: 8001 samples\n",
      "Training loss (for one batch) at step 8200: 0.1536\n",
      "Seen so far: 8201 samples\n",
      "Training loss (for one batch) at step 8400: 0.1526\n",
      "Seen so far: 8401 samples\n",
      "Training loss (for one batch) at step 8600: 0.1510\n",
      "Seen so far: 8601 samples\n",
      "Training loss (for one batch) at step 8800: 0.1497\n",
      "Seen so far: 8801 samples\n",
      "Training loss (for one batch) at step 9000: 0.1483\n",
      "Seen so far: 9001 samples\n",
      "Training loss (for one batch) at step 9200: 0.1470\n",
      "Seen so far: 9201 samples\n",
      "Training loss (for one batch) at step 9400: 0.1459\n",
      "Seen so far: 9401 samples\n",
      "Training loss (for one batch) at step 9600: 0.1442\n",
      "Seen so far: 9601 samples\n",
      "Training loss (for one batch) at step 9800: 0.1436\n",
      "Seen so far: 9801 samples\n",
      "Training loss (for one batch) at step 10000: 82401872.0000\n",
      "Seen so far: 10001 samples\n",
      "Training loss (for one batch) at step 10200: 0.1406\n",
      "Seen so far: 10201 samples\n",
      "Training loss (for one batch) at step 10400: 0.1394\n",
      "Seen so far: 10401 samples\n",
      "Training loss (for one batch) at step 10600: 80020504.0000\n",
      "Seen so far: 10601 samples\n",
      "Training loss (for one batch) at step 10800: 0.1372\n",
      "Seen so far: 10801 samples\n",
      "Training loss (for one batch) at step 11000: 0.1361\n",
      "Seen so far: 11001 samples\n",
      "Training loss (for one batch) at step 11200: 0.1350\n",
      "Seen so far: 11201 samples\n",
      "Training loss (for one batch) at step 11400: 77700608.0000\n",
      "Seen so far: 11401 samples\n",
      "Training loss (for one batch) at step 11600: 0.1693\n",
      "Seen so far: 11601 samples\n",
      "Training loss (for one batch) at step 11800: 0.1315\n",
      "Seen so far: 11801 samples\n",
      "Training loss (for one batch) at step 12000: 0.1388\n",
      "Seen so far: 12001 samples\n",
      "Training loss (for one batch) at step 12200: 0.1302\n",
      "Seen so far: 12201 samples\n",
      "Training loss (for one batch) at step 12400: 74054128.0000\n",
      "Seen so far: 12401 samples\n",
      "Training loss (for one batch) at step 12600: 0.1282\n",
      "Seen so far: 12601 samples\n",
      "Training loss (for one batch) at step 12800: 0.1275\n",
      "Seen so far: 12801 samples\n",
      "Training loss (for one batch) at step 13000: 0.1266\n",
      "Seen so far: 13001 samples\n",
      "Training loss (for one batch) at step 13200: 0.1255\n",
      "Seen so far: 13201 samples\n",
      "Training loss (for one batch) at step 13400: 0.1249\n",
      "Seen so far: 13401 samples\n",
      "Training loss (for one batch) at step 13600: 0.1238\n",
      "Seen so far: 13601 samples\n",
      "Training loss (for one batch) at step 13800: 0.1228\n",
      "Seen so far: 13801 samples\n",
      "Training loss (for one batch) at step 14000: 0.1222\n",
      "Seen so far: 14001 samples\n",
      "Training loss (for one batch) at step 14200: 0.1212\n",
      "Seen so far: 14201 samples\n",
      "Training loss (for one batch) at step 14400: 0.1201\n",
      "Seen so far: 14401 samples\n",
      "Training loss (for one batch) at step 14600: 0.1192\n",
      "Seen so far: 14601 samples\n",
      "Training loss (for one batch) at step 14800: 0.8951\n",
      "Seen so far: 14801 samples\n",
      "Training loss (for one batch) at step 15000: 0.1178\n",
      "Seen so far: 15001 samples\n",
      "Training loss (for one batch) at step 15200: 0.1167\n",
      "Seen so far: 15201 samples\n",
      "Training loss (for one batch) at step 15400: 1416899840.0000\n",
      "Seen so far: 15401 samples\n",
      "Training loss (for one batch) at step 15600: 0.1153\n",
      "Seen so far: 15601 samples\n",
      "Training loss (for one batch) at step 15800: 0.1147\n",
      "Seen so far: 15801 samples\n",
      "Training loss (for one batch) at step 16000: 0.1133\n",
      "Seen so far: 16001 samples\n",
      "Training loss (for one batch) at step 16200: 0.1125\n",
      "Seen so far: 16201 samples\n",
      "Training loss (for one batch) at step 16400: 0.1122\n",
      "Seen so far: 16401 samples\n",
      "Training loss (for one batch) at step 16600: 0.1119\n",
      "Seen so far: 16601 samples\n",
      "Training loss (for one batch) at step 16800: 0.4616\n",
      "Seen so far: 16801 samples\n",
      "Training loss (for one batch) at step 17000: 0.1100\n",
      "Seen so far: 17001 samples\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 63140160.0000\n",
      "Seen so far: 1 samples\n",
      "Training loss (for one batch) at step 200: 0.1085\n",
      "Seen so far: 201 samples\n",
      "Training loss (for one batch) at step 400: 0.1077\n",
      "Seen so far: 401 samples\n",
      "Training loss (for one batch) at step 600: 0.1069\n",
      "Seen so far: 601 samples\n",
      "Training loss (for one batch) at step 800: 0.1061\n",
      "Seen so far: 801 samples\n",
      "Training loss (for one batch) at step 1000: 0.1057\n",
      "Seen so far: 1001 samples\n",
      "Training loss (for one batch) at step 1200: 0.1050\n",
      "Seen so far: 1201 samples\n",
      "Training loss (for one batch) at step 1400: 0.1044\n",
      "Seen so far: 1401 samples\n",
      "Training loss (for one batch) at step 1600: 0.1036\n",
      "Seen so far: 1601 samples\n",
      "Training loss (for one batch) at step 1800: 0.1022\n",
      "Seen so far: 1801 samples\n",
      "Training loss (for one batch) at step 2000: 0.1021\n",
      "Seen so far: 2001 samples\n",
      "Training loss (for one batch) at step 2200: 0.1016\n",
      "Seen so far: 2201 samples\n",
      "Training loss (for one batch) at step 2400: 0.1010\n",
      "Seen so far: 2401 samples\n",
      "Training loss (for one batch) at step 2600: 0.0998\n",
      "Seen so far: 2601 samples\n",
      "Training loss (for one batch) at step 2800: 0.0996\n",
      "Seen so far: 2801 samples\n",
      "Training loss (for one batch) at step 3000: 0.0989\n",
      "Seen so far: 3001 samples\n",
      "Training loss (for one batch) at step 3200: 0.0984\n",
      "Seen so far: 3201 samples\n",
      "Training loss (for one batch) at step 3400: 0.0975\n",
      "Seen so far: 3401 samples\n",
      "Training loss (for one batch) at step 3600: 0.0968\n",
      "Seen so far: 3601 samples\n",
      "Training loss (for one batch) at step 3800: 0.0965\n",
      "Seen so far: 3801 samples\n",
      "Training loss (for one batch) at step 4000: 0.0959\n",
      "Seen so far: 4001 samples\n",
      "Training loss (for one batch) at step 4200: 0.0953\n",
      "Seen so far: 4201 samples\n",
      "Training loss (for one batch) at step 4400: 0.0945\n",
      "Seen so far: 4401 samples\n",
      "Training loss (for one batch) at step 4600: 0.0940\n",
      "Seen so far: 4601 samples\n",
      "Training loss (for one batch) at step 4800: 0.0934\n",
      "Seen so far: 4801 samples\n",
      "Training loss (for one batch) at step 5000: 0.0934\n",
      "Seen so far: 5001 samples\n",
      "Training loss (for one batch) at step 5200: 0.0927\n",
      "Seen so far: 5201 samples\n",
      "Training loss (for one batch) at step 5400: 0.0918\n",
      "Seen so far: 5401 samples\n",
      "Training loss (for one batch) at step 5600: 0.0914\n",
      "Seen so far: 5601 samples\n",
      "Training loss (for one batch) at step 5800: 0.0910\n",
      "Seen so far: 5801 samples\n",
      "Training loss (for one batch) at step 6000: 0.0901\n",
      "Seen so far: 6001 samples\n",
      "Training loss (for one batch) at step 6200: 0.0898\n",
      "Seen so far: 6201 samples\n",
      "Training loss (for one batch) at step 6400: 0.0886\n",
      "Seen so far: 6401 samples\n",
      "Training loss (for one batch) at step 6600: 0.0879\n",
      "Seen so far: 6601 samples\n",
      "Training loss (for one batch) at step 6800: 0.0880\n",
      "Seen so far: 6801 samples\n",
      "Training loss (for one batch) at step 7000: 0.0872\n",
      "Seen so far: 7001 samples\n",
      "Training loss (for one batch) at step 7200: 0.0868\n",
      "Seen so far: 7201 samples\n",
      "Training loss (for one batch) at step 7400: 0.0864\n",
      "Seen so far: 7401 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 7600: 0.0858\n",
      "Seen so far: 7601 samples\n",
      "Training loss (for one batch) at step 7800: 0.0853\n",
      "Seen so far: 7801 samples\n",
      "Training loss (for one batch) at step 8000: 0.0852\n",
      "Seen so far: 8001 samples\n",
      "Training loss (for one batch) at step 8200: 0.0844\n",
      "Seen so far: 8201 samples\n",
      "Training loss (for one batch) at step 8400: 0.0838\n",
      "Seen so far: 8401 samples\n",
      "Training loss (for one batch) at step 8600: 0.0836\n",
      "Seen so far: 8601 samples\n",
      "Training loss (for one batch) at step 8800: 0.0830\n",
      "Seen so far: 8801 samples\n",
      "Training loss (for one batch) at step 9000: 0.0826\n",
      "Seen so far: 9001 samples\n",
      "Training loss (for one batch) at step 9200: 0.0819\n",
      "Seen so far: 9201 samples\n",
      "Training loss (for one batch) at step 9400: 0.0815\n",
      "Seen so far: 9401 samples\n",
      "Training loss (for one batch) at step 9600: 0.0812\n",
      "Seen so far: 9601 samples\n",
      "Training loss (for one batch) at step 9800: 0.0802\n",
      "Seen so far: 9801 samples\n",
      "Training loss (for one batch) at step 10000: 25072124.0000\n",
      "Seen so far: 10001 samples\n",
      "Training loss (for one batch) at step 10200: 0.0795\n",
      "Seen so far: 10201 samples\n",
      "Training loss (for one batch) at step 10400: 0.0789\n",
      "Seen so far: 10401 samples\n",
      "Training loss (for one batch) at step 10600: 22663388.0000\n",
      "Seen so far: 10601 samples\n",
      "Training loss (for one batch) at step 10800: 0.0782\n",
      "Seen so far: 10801 samples\n",
      "Training loss (for one batch) at step 11000: 0.0775\n",
      "Seen so far: 11001 samples\n",
      "Training loss (for one batch) at step 11200: 0.0770\n",
      "Seen so far: 11201 samples\n",
      "Training loss (for one batch) at step 11400: 20308430.0000\n",
      "Seen so far: 11401 samples\n",
      "Training loss (for one batch) at step 11600: 0.0941\n",
      "Seen so far: 11601 samples\n",
      "Training loss (for one batch) at step 11800: 0.0755\n",
      "Seen so far: 11801 samples\n",
      "Training loss (for one batch) at step 12000: 0.0757\n",
      "Seen so far: 12001 samples\n",
      "Training loss (for one batch) at step 12200: 0.0746\n",
      "Seen so far: 12201 samples\n",
      "Training loss (for one batch) at step 12400: 16599533.0000\n",
      "Seen so far: 12401 samples\n",
      "Training loss (for one batch) at step 12600: 0.0736\n",
      "Seen so far: 12601 samples\n",
      "Training loss (for one batch) at step 12800: 0.0733\n",
      "Seen so far: 12801 samples\n",
      "Training loss (for one batch) at step 13000: 0.0727\n",
      "Seen so far: 13001 samples\n",
      "Training loss (for one batch) at step 13200: 0.0725\n",
      "Seen so far: 13201 samples\n",
      "Training loss (for one batch) at step 13400: 0.0720\n",
      "Seen so far: 13401 samples\n",
      "Training loss (for one batch) at step 13600: 0.0715\n",
      "Seen so far: 13601 samples\n",
      "Training loss (for one batch) at step 13800: 0.0710\n",
      "Seen so far: 13801 samples\n",
      "Training loss (for one batch) at step 14000: 0.0707\n",
      "Seen so far: 14001 samples\n",
      "Training loss (for one batch) at step 14200: 0.0701\n",
      "Seen so far: 14201 samples\n",
      "Training loss (for one batch) at step 14400: 0.0698\n",
      "Seen so far: 14401 samples\n",
      "Training loss (for one batch) at step 14600: 0.0692\n",
      "Seen so far: 14601 samples\n",
      "Training loss (for one batch) at step 14800: 1.0903\n",
      "Seen so far: 14801 samples\n",
      "Training loss (for one batch) at step 15000: 0.0684\n",
      "Seen so far: 15001 samples\n",
      "Training loss (for one batch) at step 15200: 0.0679\n",
      "Seen so far: 15201 samples\n",
      "Training loss (for one batch) at step 15400: 259564384.0000\n",
      "Seen so far: 15401 samples\n",
      "Training loss (for one batch) at step 15600: 0.0676\n",
      "Seen so far: 15601 samples\n",
      "Training loss (for one batch) at step 15800: 0.0669\n",
      "Seen so far: 15801 samples\n",
      "Training loss (for one batch) at step 16000: 0.0666\n",
      "Seen so far: 16001 samples\n",
      "Training loss (for one batch) at step 16200: 0.0665\n",
      "Seen so far: 16201 samples\n",
      "Training loss (for one batch) at step 16400: 0.0661\n",
      "Seen so far: 16401 samples\n",
      "Training loss (for one batch) at step 16600: 0.0654\n",
      "Seen so far: 16601 samples\n",
      "Training loss (for one batch) at step 16800: 0.4882\n",
      "Seen so far: 16801 samples\n",
      "Training loss (for one batch) at step 17000: 0.0647\n",
      "Seen so far: 17001 samples\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 5309914.5000\n",
      "Seen so far: 1 samples\n",
      "Training loss (for one batch) at step 200: 0.0640\n",
      "Seen so far: 201 samples\n",
      "Training loss (for one batch) at step 400: 0.0635\n",
      "Seen so far: 401 samples\n",
      "Training loss (for one batch) at step 600: 0.0633\n",
      "Seen so far: 601 samples\n",
      "Training loss (for one batch) at step 800: 0.0626\n",
      "Seen so far: 801 samples\n",
      "Training loss (for one batch) at step 1000: 0.0625\n",
      "Seen so far: 1001 samples\n",
      "Training loss (for one batch) at step 1200: 0.0626\n",
      "Seen so far: 1201 samples\n",
      "Training loss (for one batch) at step 1400: 0.0622\n",
      "Seen so far: 1401 samples\n",
      "Training loss (for one batch) at step 1600: 0.0617\n",
      "Seen so far: 1601 samples\n",
      "Training loss (for one batch) at step 1800: 0.0611\n",
      "Seen so far: 1801 samples\n",
      "Training loss (for one batch) at step 2000: 0.0610\n",
      "Seen so far: 2001 samples\n",
      "Training loss (for one batch) at step 2200: 0.0605\n",
      "Seen so far: 2201 samples\n",
      "Training loss (for one batch) at step 2400: 0.0597\n",
      "Seen so far: 2401 samples\n",
      "Training loss (for one batch) at step 2600: 0.0594\n",
      "Seen so far: 2601 samples\n",
      "Training loss (for one batch) at step 2800: 0.0588\n",
      "Seen so far: 2801 samples\n",
      "Training loss (for one batch) at step 3000: 0.0581\n",
      "Seen so far: 3001 samples\n",
      "Training loss (for one batch) at step 3200: 0.0581\n",
      "Seen so far: 3201 samples\n",
      "Training loss (for one batch) at step 3400: 0.0579\n",
      "Seen so far: 3401 samples\n",
      "Training loss (for one batch) at step 3600: 0.0575\n",
      "Seen so far: 3601 samples\n",
      "Training loss (for one batch) at step 3800: 0.0567\n",
      "Seen so far: 3801 samples\n",
      "Training loss (for one batch) at step 4000: 0.0563\n",
      "Seen so far: 4001 samples\n",
      "Training loss (for one batch) at step 4200: 0.0562\n",
      "Seen so far: 4201 samples\n",
      "Training loss (for one batch) at step 4400: 0.0560\n",
      "Seen so far: 4401 samples\n",
      "Training loss (for one batch) at step 4600: 0.0554\n",
      "Seen so far: 4601 samples\n",
      "Training loss (for one batch) at step 4800: 0.0546\n",
      "Seen so far: 4801 samples\n",
      "Training loss (for one batch) at step 5000: 0.0545\n",
      "Seen so far: 5001 samples\n",
      "Training loss (for one batch) at step 5200: 0.0536\n",
      "Seen so far: 5201 samples\n",
      "Training loss (for one batch) at step 5400: 0.0535\n",
      "Seen so far: 5401 samples\n",
      "Training loss (for one batch) at step 5600: 0.0532\n",
      "Seen so far: 5601 samples\n",
      "Training loss (for one batch) at step 5800: 0.0525\n",
      "Seen so far: 5801 samples\n",
      "Training loss (for one batch) at step 6000: 0.0521\n",
      "Seen so far: 6001 samples\n",
      "Training loss (for one batch) at step 6200: 0.0516\n",
      "Seen so far: 6201 samples\n",
      "Training loss (for one batch) at step 6400: 0.0508\n",
      "Seen so far: 6401 samples\n",
      "Training loss (for one batch) at step 6600: 0.0509\n",
      "Seen so far: 6601 samples\n",
      "Training loss (for one batch) at step 6800: 0.0502\n",
      "Seen so far: 6801 samples\n",
      "Training loss (for one batch) at step 7000: 0.0497\n",
      "Seen so far: 7001 samples\n",
      "Training loss (for one batch) at step 7200: 0.0498\n",
      "Seen so far: 7201 samples\n",
      "Training loss (for one batch) at step 7400: 0.0495\n",
      "Seen so far: 7401 samples\n",
      "Training loss (for one batch) at step 7600: 0.0493\n",
      "Seen so far: 7601 samples\n",
      "Training loss (for one batch) at step 7800: 0.0491\n",
      "Seen so far: 7801 samples\n",
      "Training loss (for one batch) at step 8000: 0.0487\n",
      "Seen so far: 8001 samples\n",
      "Training loss (for one batch) at step 8200: 0.0483\n",
      "Seen so far: 8201 samples\n",
      "Training loss (for one batch) at step 8400: 0.0482\n",
      "Seen so far: 8401 samples\n",
      "Training loss (for one batch) at step 8600: 0.0475\n",
      "Seen so far: 8601 samples\n",
      "Training loss (for one batch) at step 8800: 0.0474\n",
      "Seen so far: 8801 samples\n",
      "Training loss (for one batch) at step 9000: 0.0471\n",
      "Seen so far: 9001 samples\n",
      "Training loss (for one batch) at step 9200: 0.0468\n",
      "Seen so far: 9201 samples\n",
      "Training loss (for one batch) at step 9400: 0.0465\n",
      "Seen so far: 9401 samples\n",
      "Training loss (for one batch) at step 9600: 0.0458\n",
      "Seen so far: 9601 samples\n",
      "Training loss (for one batch) at step 9800: 0.0457\n",
      "Seen so far: 9801 samples\n",
      "Training loss (for one batch) at step 10000: 0.0453\n",
      "Seen so far: 10001 samples\n",
      "Training loss (for one batch) at step 10200: 0.0452\n",
      "Seen so far: 10201 samples\n",
      "Training loss (for one batch) at step 10400: 0.0448\n",
      "Seen so far: 10401 samples\n",
      "Training loss (for one batch) at step 10600: 0.0446\n",
      "Seen so far: 10601 samples\n",
      "Training loss (for one batch) at step 10800: 0.0440\n",
      "Seen so far: 10801 samples\n",
      "Training loss (for one batch) at step 11000: 0.0441\n",
      "Seen so far: 11001 samples\n",
      "Training loss (for one batch) at step 11200: 0.0434\n",
      "Seen so far: 11201 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 11400: 0.0436\n",
      "Seen so far: 11401 samples\n",
      "Training loss (for one batch) at step 11600: 0.0549\n",
      "Seen so far: 11601 samples\n",
      "Training loss (for one batch) at step 11800: 0.0427\n",
      "Seen so far: 11801 samples\n",
      "Training loss (for one batch) at step 12000: 0.0421\n",
      "Seen so far: 12001 samples\n",
      "Training loss (for one batch) at step 12200: 0.0419\n",
      "Seen so far: 12201 samples\n",
      "Training loss (for one batch) at step 12400: 0.0417\n",
      "Seen so far: 12401 samples\n",
      "Training loss (for one batch) at step 12600: 0.0415\n",
      "Seen so far: 12601 samples\n",
      "Training loss (for one batch) at step 12800: 0.0413\n",
      "Seen so far: 12801 samples\n",
      "Training loss (for one batch) at step 13000: 0.0410\n",
      "Seen so far: 13001 samples\n",
      "Training loss (for one batch) at step 13200: 0.0411\n",
      "Seen so far: 13201 samples\n",
      "Training loss (for one batch) at step 13400: 0.0405\n",
      "Seen so far: 13401 samples\n",
      "Training loss (for one batch) at step 13600: 0.0402\n",
      "Seen so far: 13601 samples\n",
      "Training loss (for one batch) at step 13800: 0.0399\n",
      "Seen so far: 13801 samples\n",
      "Training loss (for one batch) at step 14000: 0.0396\n",
      "Seen so far: 14001 samples\n",
      "Training loss (for one batch) at step 14200: 0.0395\n",
      "Seen so far: 14201 samples\n",
      "Training loss (for one batch) at step 14400: 0.0392\n",
      "Seen so far: 14401 samples\n",
      "Training loss (for one batch) at step 14600: 0.0388\n",
      "Seen so far: 14601 samples\n",
      "Training loss (for one batch) at step 14800: 1.3312\n",
      "Seen so far: 14801 samples\n",
      "Training loss (for one batch) at step 15000: 0.0383\n",
      "Seen so far: 15001 samples\n",
      "Training loss (for one batch) at step 15200: 0.0384\n",
      "Seen so far: 15201 samples\n",
      "Training loss (for one batch) at step 15400: 0.0382\n",
      "Seen so far: 15401 samples\n",
      "Training loss (for one batch) at step 15600: 0.0377\n",
      "Seen so far: 15601 samples\n",
      "Training loss (for one batch) at step 15800: 0.0375\n",
      "Seen so far: 15801 samples\n",
      "Training loss (for one batch) at step 16000: 0.0372\n",
      "Seen so far: 16001 samples\n",
      "Training loss (for one batch) at step 16200: 0.0370\n",
      "Seen so far: 16201 samples\n",
      "Training loss (for one batch) at step 16400: 0.0370\n",
      "Seen so far: 16401 samples\n",
      "Training loss (for one batch) at step 16600: 0.0366\n",
      "Seen so far: 16601 samples\n",
      "Training loss (for one batch) at step 16800: 0.5730\n",
      "Seen so far: 16801 samples\n",
      "Training loss (for one batch) at step 17000: 0.0361\n",
      "Seen so far: 17001 samples\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.0360\n",
      "Seen so far: 1 samples\n",
      "Training loss (for one batch) at step 200: 0.0355\n",
      "Seen so far: 201 samples\n",
      "Training loss (for one batch) at step 400: 0.0355\n",
      "Seen so far: 401 samples\n",
      "Training loss (for one batch) at step 600: 0.0356\n",
      "Seen so far: 601 samples\n",
      "Training loss (for one batch) at step 800: 0.0352\n",
      "Seen so far: 801 samples\n",
      "Training loss (for one batch) at step 1000: 0.0348\n",
      "Seen so far: 1001 samples\n",
      "Training loss (for one batch) at step 1200: 0.0349\n",
      "Seen so far: 1201 samples\n",
      "Training loss (for one batch) at step 1400: 0.0344\n",
      "Seen so far: 1401 samples\n",
      "Training loss (for one batch) at step 1600: 0.0346\n",
      "Seen so far: 1601 samples\n",
      "Training loss (for one batch) at step 1800: 0.0342\n",
      "Seen so far: 1801 samples\n",
      "Training loss (for one batch) at step 2000: 0.0341\n",
      "Seen so far: 2001 samples\n",
      "Training loss (for one batch) at step 2200: 0.0340\n",
      "Seen so far: 2201 samples\n",
      "Training loss (for one batch) at step 2400: 0.0337\n",
      "Seen so far: 2401 samples\n",
      "Training loss (for one batch) at step 2600: 0.0337\n",
      "Seen so far: 2601 samples\n",
      "Training loss (for one batch) at step 2800: 0.0335\n",
      "Seen so far: 2801 samples\n",
      "Training loss (for one batch) at step 3000: 0.0332\n",
      "Seen so far: 3001 samples\n",
      "Training loss (for one batch) at step 3200: 0.0331\n",
      "Seen so far: 3201 samples\n",
      "Training loss (for one batch) at step 3400: 0.0331\n",
      "Seen so far: 3401 samples\n",
      "Training loss (for one batch) at step 3600: 0.0328\n",
      "Seen so far: 3601 samples\n",
      "Training loss (for one batch) at step 3800: 0.0328\n",
      "Seen so far: 3801 samples\n",
      "Training loss (for one batch) at step 4000: 0.0323\n",
      "Seen so far: 4001 samples\n",
      "Training loss (for one batch) at step 4200: 0.0322\n",
      "Seen so far: 4201 samples\n",
      "Training loss (for one batch) at step 4400: 0.0322\n",
      "Seen so far: 4401 samples\n",
      "Training loss (for one batch) at step 4600: 0.0321\n",
      "Seen so far: 4601 samples\n",
      "Training loss (for one batch) at step 4800: 0.0320\n",
      "Seen so far: 4801 samples\n",
      "Training loss (for one batch) at step 5000: 0.0316\n",
      "Seen so far: 5001 samples\n",
      "Training loss (for one batch) at step 5200: 0.0315\n",
      "Seen so far: 5201 samples\n",
      "Training loss (for one batch) at step 5400: 0.0312\n",
      "Seen so far: 5401 samples\n",
      "Training loss (for one batch) at step 5600: 0.0314\n",
      "Seen so far: 5601 samples\n",
      "Training loss (for one batch) at step 5800: 0.0310\n",
      "Seen so far: 5801 samples\n",
      "Training loss (for one batch) at step 6000: 0.0311\n",
      "Seen so far: 6001 samples\n",
      "Training loss (for one batch) at step 6200: 0.0309\n",
      "Seen so far: 6201 samples\n",
      "Training loss (for one batch) at step 6400: 0.0304\n",
      "Seen so far: 6401 samples\n",
      "Training loss (for one batch) at step 6600: 0.0306\n",
      "Seen so far: 6601 samples\n",
      "Training loss (for one batch) at step 6800: 0.0306\n",
      "Seen so far: 6801 samples\n",
      "Training loss (for one batch) at step 7000: 0.0302\n",
      "Seen so far: 7001 samples\n",
      "Training loss (for one batch) at step 7200: 0.0301\n",
      "Seen so far: 7201 samples\n",
      "Training loss (for one batch) at step 7400: 0.0301\n",
      "Seen so far: 7401 samples\n",
      "Training loss (for one batch) at step 7600: 0.0300\n",
      "Seen so far: 7601 samples\n",
      "Training loss (for one batch) at step 7800: 0.0295\n",
      "Seen so far: 7801 samples\n",
      "Training loss (for one batch) at step 8000: 0.0294\n",
      "Seen so far: 8001 samples\n",
      "Training loss (for one batch) at step 8200: 0.0294\n",
      "Seen so far: 8201 samples\n",
      "Training loss (for one batch) at step 8400: 0.0289\n",
      "Seen so far: 8401 samples\n",
      "Training loss (for one batch) at step 8600: 0.0292\n",
      "Seen so far: 8601 samples\n",
      "Training loss (for one batch) at step 8800: 0.0290\n",
      "Seen so far: 8801 samples\n",
      "Training loss (for one batch) at step 9000: 0.0286\n",
      "Seen so far: 9001 samples\n",
      "Training loss (for one batch) at step 9200: 0.0286\n",
      "Seen so far: 9201 samples\n",
      "Training loss (for one batch) at step 9400: 0.0286\n",
      "Seen so far: 9401 samples\n",
      "Training loss (for one batch) at step 9600: 0.0284\n",
      "Seen so far: 9601 samples\n",
      "Training loss (for one batch) at step 9800: 0.0281\n",
      "Seen so far: 9801 samples\n",
      "Training loss (for one batch) at step 10000: 0.0280\n",
      "Seen so far: 10001 samples\n",
      "Training loss (for one batch) at step 10200: 0.0278\n",
      "Seen so far: 10201 samples\n",
      "Training loss (for one batch) at step 10400: 0.0276\n",
      "Seen so far: 10401 samples\n",
      "Training loss (for one batch) at step 10600: 0.0273\n",
      "Seen so far: 10601 samples\n",
      "Training loss (for one batch) at step 10800: 0.0273\n",
      "Seen so far: 10801 samples\n",
      "Training loss (for one batch) at step 11000: 0.0269\n",
      "Seen so far: 11001 samples\n",
      "Training loss (for one batch) at step 11200: 0.0271\n",
      "Seen so far: 11201 samples\n",
      "Training loss (for one batch) at step 11400: 0.0267\n",
      "Seen so far: 11401 samples\n",
      "Training loss (for one batch) at step 11600: 0.0358\n",
      "Seen so far: 11601 samples\n",
      "Training loss (for one batch) at step 11800: 0.0263\n",
      "Seen so far: 11801 samples\n",
      "Training loss (for one batch) at step 12000: 0.0261\n",
      "Seen so far: 12001 samples\n",
      "Training loss (for one batch) at step 12200: 0.0261\n",
      "Seen so far: 12201 samples\n",
      "Training loss (for one batch) at step 12400: 0.0258\n",
      "Seen so far: 12401 samples\n",
      "Training loss (for one batch) at step 12600: 0.0258\n",
      "Seen so far: 12601 samples\n",
      "Training loss (for one batch) at step 12800: 0.0254\n",
      "Seen so far: 12801 samples\n",
      "Training loss (for one batch) at step 13000: 0.0252\n",
      "Seen so far: 13001 samples\n",
      "Training loss (for one batch) at step 13200: 0.0250\n",
      "Seen so far: 13201 samples\n",
      "Training loss (for one batch) at step 13400: 0.0248\n",
      "Seen so far: 13401 samples\n",
      "Training loss (for one batch) at step 13600: 0.0251\n",
      "Seen so far: 13601 samples\n",
      "Training loss (for one batch) at step 13800: 0.0247\n",
      "Seen so far: 13801 samples\n",
      "Training loss (for one batch) at step 14000: 0.0246\n",
      "Seen so far: 14001 samples\n",
      "Training loss (for one batch) at step 14200: 0.0243\n",
      "Seen so far: 14201 samples\n",
      "Training loss (for one batch) at step 14400: 0.0244\n",
      "Seen so far: 14401 samples\n",
      "Training loss (for one batch) at step 14600: 0.0240\n",
      "Seen so far: 14601 samples\n",
      "Training loss (for one batch) at step 14800: 1.5471\n",
      "Seen so far: 14801 samples\n",
      "Training loss (for one batch) at step 15000: 0.0239\n",
      "Seen so far: 15001 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 15200: 0.0238\n",
      "Seen so far: 15201 samples\n",
      "Training loss (for one batch) at step 15400: 0.0237\n",
      "Seen so far: 15401 samples\n",
      "Training loss (for one batch) at step 15600: 0.0232\n",
      "Seen so far: 15601 samples\n",
      "Training loss (for one batch) at step 15800: 0.0232\n",
      "Seen so far: 15801 samples\n",
      "Training loss (for one batch) at step 16000: 0.0230\n",
      "Seen so far: 16001 samples\n",
      "Training loss (for one batch) at step 16200: 0.0231\n",
      "Seen so far: 16201 samples\n",
      "Training loss (for one batch) at step 16400: 0.0225\n",
      "Seen so far: 16401 samples\n",
      "Training loss (for one batch) at step 16600: 0.0223\n",
      "Seen so far: 16601 samples\n",
      "Training loss (for one batch) at step 16800: 0.6221\n",
      "Seen so far: 16801 samples\n",
      "Training loss (for one batch) at step 17000: 0.0220\n",
      "Seen so far: 17001 samples\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# TRAINING\n",
    "model = get_model(True)\n",
    "\n",
    "# HYPERPARAMETER SETTING\n",
    "epochs = 5\n",
    "learning_rt = 1e-5\n",
    "batch_size=1\n",
    "seed=60\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#Early stopping (disabled by default)\n",
    "# Threshold for training loss\n",
    "es=False;\n",
    "earlystop_thresh=1e-3;\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "flag = False;\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prev_loss=0.0\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for i in range(0,train_X.shape[0]):\n",
    "        x_batch_train=np.expand_dims(train_X[i],0)\n",
    "        y_batch_train=train_y[i]\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if i % 200 == 0:\n",
    "            if (abs(loss_value.numpy()-prev_loss)< earlystop_thresh and es == True):\n",
    "                flag=True\n",
    "                break;\n",
    "            prev_loss = loss_value.numpy()\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (i, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((i + 1) * batch_size))\n",
    "            \n",
    "    if flag == True:\n",
    "        print(\"\\nStopped on %d\" % (epoch,))\n",
    "        print(\"Loss below threshold\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf9e17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SWITCH TO INFERENCE FOR COMPUTING PREDICTIONS\n",
    "## (TURN OFF CONCATENATION)\n",
    "inference_mdl = get_model(train=False)\n",
    "inference_mdl.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95277e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (inference_mdl.predict(test_data))\n",
    "rounded = np.round(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce572776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs 5\n",
      "Learning Rate 1e-05\n",
      "Accuracy:  0.9669376693766938\n",
      "Precision:  0.9467425025853154\n",
      "F1 Score:  0.9677589852008457\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of epochs\", epochs)\n",
    "print(\"Learning Rate\",learning_rt)\n",
    "print('Accuracy: ',accuracy_score(np.concatenate(test_labels),np.concatenate(rounded)))\n",
    "print('Precision: ',precision_score(np.concatenate(test_labels),np.concatenate(rounded)))\n",
    "print('F1 Score: ', f1_score(np.concatenate(test_labels),np.concatenate(rounded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fded93ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(np.concatenate(test_labels), np.concatenate(rounded))\n",
    "plt.figure(figsize=(7,7))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['not Mal','Mal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a7ece3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: malicious_refined2/assets\n"
     ]
    }
   ],
   "source": [
    "inference_mdl.save('malicious_refined2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aba36b",
   "metadata": {},
   "source": [
    "# OC-SVM\n",
    "\n",
    "One class svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50eb4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch malware samples for testing\n",
    "seed = 50\n",
    "bigDF =prepareData(\"C:/Users/aweso/Downloads/ALL_important/\")\n",
    "bigDF_labels = np.array([1.0 for i in range(0,bigDF.shape[0])])\n",
    "x_train,x_test,_,y_test = train_test_split(bigDF,bigDF_labels,test_size=0.2,random_state=seed) \n",
    "\n",
    "# Load benign data in\n",
    "test_benign = spark.read.format(\"json\").option(\"inferSchema\",True).load(\"C:/Users/aweso/Downloads/ben_lz_all_important/benign3/part-00000-5d4fa1ac-c21e-4ccb-a04f-4d173e78d260-c000.json\").toPandas()\n",
    "benign_test_labels = np.array([-1.0 for i in range(0,test_benign.shape[0])])\n",
    "\n",
    "test_mal = resample(x_test,n_samples=benign_test_labels.shape[0], random_state=seed)\n",
    "test_mal_labels = np.array([1.0 for i in range(0,test_mal.shape[0])])\n",
    "# Concatenate the data for testing\n",
    "test_data = pd.DataFrame(np.concatenate([test_mal,test_benign],axis=0))\n",
    "test_labels = pd.DataFrame(np.concatenate([test_mal_labels,benign_test_labels]),columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18e82c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneClassSVM(gamma='auto', nu=0.01, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.OneClassSVM(nu=0.01,kernel='rbf', gamma='auto', verbose=1)\n",
    "clf.fit(x_train.fillna(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ee3dad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but OneClassSVM was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6990265008112493\n",
      "Precision:  0.6671207992733879\n",
      "F1:  0.6672645739910315\n"
     ]
    }
   ],
   "source": [
    "# test_labels = pd.concat([zeros_DF,onesTest_DF],axis=0)\n",
    "yhat = clf.predict(test_data)\n",
    "\n",
    "print('Accuracy: ',accuracy_score(test_labels,yhat))\n",
    "print('Precision: ',precision_score(test_labels,yhat))\n",
    "print('F1: ', f1_score(test_labels,yhat,pos_label=-1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ad070e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(test_labels, yhat)\n",
    "plt.figure(figsize=(7,7))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['not Mal','Mal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f86fe435",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'all.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
