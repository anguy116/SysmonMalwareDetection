# Detection-of-Malware-execution-using-Sysmon-Logs

#### Config Setup
##### IMPORTANT:
You need to change this accordingly prior to building the `sysmon-ml` docker container

You will need to change the `config.json` to fit the model you choose to predict malicious network traffic. 

   1. `model_type`: `neural-net` or `svm`
   2. `model_path`: absolute (or relative) path to the model
      1. `neural-net`: root folder of model's `assets` and `variables`
      2. `svm`: `.sav` file for chosen model
   3. `elastic_index`: elasticsearch index model predictions will be sent to
   4. `threshold`: (neural-net only) floating point probability that will be considered a malicious event
   5. `processing_type`:
      1. `all`: svm and some models (will produce 44 fields for prediction) or generic network
      2. `ransom`: for ransomware neural networks
      3. `trojan`: for trojan neural networks
      
ex:
```json
{
  "model_type" : "neural-net",
  "model_path": "model/model_weights/mal/all.sav",
  "elastic_index": "malicious_predictions_nn_all",
  "threshold": 0.5,
  "processing_type": "all"
}
```

### Running the malware ingestion/prediction application

The goal is to run this in conjunction with Elastic Winlogbeat

1. In the ubuntu machine check if the ens33 ip address is available
   1. If it isn't there run: `sudo ip link set dev ens33 up` followed by `sudo dhclient -v ens33`
   2. Else, don't worry
2. To create the custom logstash container
   1. `cd logstash-raw`
   2. `sudo docker build --tag logstash-c:latest .`
3. To deploy the docker containers run `sudo docker-compose up -d` (within the project repository)
   
## Useful Docker Commands

### 1. Delete all "none" containers
`docker image rm -f $(docker images -f dangling=true -q)`

### 2. If you ever need to clear up the data that you wish to ingest:
This sets the auto delete for a kafka topic to 1 second.
`sudo docker exec -it kafka /bin/sh -c "kafka-configs.sh --alter --bootstrap-server localhost:9092 --entity-type topics --entity-name raw-dns --add-config retention.ms = 1000`
Wait a while and verify that the data has been deleted
`sudo docker exec -it kafka /bin/sh -c "kafka-console-consumer.sh --from-beginning --topic raw-dns --bootstrap-server localhost:9092`
(Pressing `ctrl+c` will display how many messages consumed during a period)
Once verified that the data is gone, reset the time to default (1 day)
`sudo docker exec -it kafka /bin/sh -c "kafka-configs.sh --alter --bootstrap-server localhost:9092 --entity-type topics --entity-name raw-dns --add-config retention.ms = 86400000`