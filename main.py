import json
import keras.models
import pandas as pd
import model.neuralnet as nn
from pyspark.sql import SparkSession
import pyspark.sql.functions as fn
from kafka import KafkaConsumer
from kafka.errors import NoBrokersAvailable
from json import loads, dumps
from preprocessing.featureExtraction import processdf
import numpy as np
import math
from elasticsearch import Elasticsearch, helpers
import pickle
from preprocessing.grabSchema import schema
from time import sleep

def split(data):
    # read benign data
    batch = []
    if (len(data) == 10 and not math.isnan(data.sum())):
        return np.asanyarray([data])
    for j in range(10, len(data), 10):
        df = data[j - 10:j].astype(float)
        if (not np.isnan(df.sum())):
            batch.append(df)
        else:
            print("found bad data")
            batch.append(np.nan_to_num(df))

    return np.asanyarray(batch)

def loadModel(type,path):
    if type == 'neural-net':
        return keras.models.load_model(path)
    else:
        with open(path,'rb') as f:
            return pickle.load(f)

def establishKafkaConnection():
    consumer=None
    conn_flag=False
    print("Waiting on Kafka connection ...")
    while not conn_flag:
        sleep(5)
        print(".")
        try:
            consumer = (KafkaConsumer(
                'raw-dns',
                bootstrap_servers=['kafka:9092'],
                value_deserializer=lambda x: loads(x.decode('utf-8')),
                auto_offset_reset='earliest'
            ))
            conn_flag = True
        except NoBrokersAvailable:
            continue
    return consumer

if __name__ == '__main__':
    # choose which model to predict with
    conf = json.load(open("config.json"))
    print("Loading config: \n",conf)

    # load the config
    model_type = conf['model_type']
    model_path = conf['model_path']
    elastic_index = conf['elastic_index']
    threshold = conf['threshold']
    processing_type = conf['processing_type']

    # load sparkSession
    spark = (SparkSession.builder
             .appName("malware_intrusion")
             .getOrCreate()
             )

    # load model
    model = loadModel(model_type,model_path)

    # load Kafka consumer
    consumer = establishKafkaConnection()

    # elastic client
    elastic_client = Elasticsearch("http://elasticsearch:9200")

    # 1. take kafka data
    raw_data = []
    if processing_type == "all":
        target_shape = (10,16)
    elif processing_type == "ransomware":
        target_shape = (10,22)
    else:
        target_shape = (10,20)
    print("Monitoring Kafka...")
    for message in consumer:
        data_val = message.value
        raw_data.append(data_val)

        if len(raw_data) >= 500:
            print("Processed enough to consume")
            # 2. process the data using spark
            df = (spark
                  .createDataFrame(raw_data, schema=schema)
                  .withColumn("hostname", fn.col("host.name"))
                  .withColumn("hosthostname", fn.col("host.hostname"))
                  .withColumn("os_name", fn.col("host.os.name"))
                  .withColumn("os_version", fn.col("host.os.version"))
                  .withColumn("ip", fn.col("host.ip")[1])
                  .withColumn("build", fn.col("host.os.build"))
                  .withColumn("type", fn.col("host.os.name"))
                  .withColumn("mac", fn.col("host.mac")[0])
                  .withColumn("architecture", fn.col("host.architecture"))
                  .withColumn("event", fn.col("event.action"))
                  .withColumn("executable", fn.col("process.name"))
                  .withColumn("md5_hash",fn.col("process.hash.md5"))
                  .withColumn("sha256_hash",fn.col("process.hash.sha256"))
                  .drop("host")
                  )

            cols = ["timestamp", "hostname", "hosthostname", "os_name", "os_version", "ip", "build", "type", "mac",
                    "architecture", "event", "executable","md5_hash","sha256_hash"]
            feature_creation = processdf(df, processing_type).toPandas()
            feature_creation_without_info = feature_creation.drop(cols, axis=1)
            info_preds = feature_creation[cols]

            # if prediction model is OC-SVM (and there are sysmon logs)
            if model_type=='svm' and feature_creation.size > 0:

                # 4. predict sysmon records
                preds = model.predict(feature_creation_without_info)

                # prepare every record that resulted in a malicious prediction
                cnt=0
                for count, pred in enumerate(preds):
                    if pred == 1:
                        cnt+=1
                        result = {
                            "timestamp": info_preds["timestamp"][count],
                            "host": info_preds["hostname"][count],
                            "host_name": info_preds["hosthostname"][count],
                            "os": info_preds["os_name"][count],
                            "os_version": info_preds["os_version"][count],
                            "ip": info_preds["ip"][count],
                            "build": info_preds["build"][count],
                            "type": info_preds["type"][count],
                            "mac": info_preds["mac"][count],
                            "architecture": info_preds["architecture"][count],
                            "event": info_preds["event"][count],
                            "executable": info_preds["executable"][count],
                            "md5_hash": info_preds["md5_hash"][count],
                            "sha256_hash": info_preds["sha256_hash"][count],
                            "prediction": str(pred),
                            "model": 'svm'
                        }
                        # 5. Push to elastic index
                        pd_df = pd.DataFrame([result]).transpose().to_dict().values()
                        helpers.bulk(elastic_client, pd_df, index=elastic_index)

                print("Total sysmon records", len(feature_creation_without_info))
                print("Total Preds: ", cnt)

            # for OC-CNN model (different fields)
            elif feature_creation.size > 0:
                print("Found sysmon logs")
    
                # if the total sysmon records are not divisible by 10 (time series)
                if feature_creation_without_info.shape[0] % 10 != 0:
                    np_pad_fill = 10 - feature_creation_without_info.shape[0] % 10

                    # pad the features
                    np_pad_zeros = np.zeros((np_pad_fill, target_shape[1]))

                    # null row
                    null_row = {
                        'timestamp': None,
                        'hostname': 'notavailable',
                        'hosthostname': 'notavailable',
                        'os_name': 'notavailable',
                        'os_family': 'notavailable',
                        'ip': 'notavailable',
                        'build': 'notavailable',
                        'type': 'notavailable',
                        'mac': 'notavailable',
                        'architecture': 'notavailable',
                        'event': 'notavailable',
                        'executable': 'notavailable',
                        'md5_hash':'notavailable',
                        'sha256_hash':'notavailable',
                    }

                    # create all filler rows necessary
                    info_preds = info_preds.append([null_row] * np_pad_fill, ignore_index=True)

                    # observations + filler rows
                    final_arr = np.concatenate([feature_creation_without_info, np_pad_zeros])
                else:
                    final_arr = np.asanyarray(feature_creation_without_info)

                # split the observation data into time-series of length 10
                nd_data = split(final_arr)

                # 4. feed model this data
                # returns confidence score and final predictions
                cs, predictions = nn.predict(model, nd_data, threshold)

                cs = cs.reshape(cs.shape[0] * cs.shape[1])
                predictions = predictions.reshape(predictions.shape[0] * predictions.shape[1])
                cnt = 0 
                for count, pred in enumerate(predictions):
                    print("CS: " + str(cs[count]))
                    if pred == 1 and info_preds['timestamp'][count] is not None:
                        cnt+=1
                        print(info_preds["timestamp"][count])
                        result = {
                            "timestamp": info_preds["timestamp"][count],
                            "host": info_preds["hostname"][count],
                            "host_name": info_preds["hosthostname"][count],
                            "os": info_preds["os_name"][count],
                            "os_version": info_preds["os_version"][count],
                            "ip": info_preds["ip"][count],
                            "build": info_preds["build"][count],
                            "type": info_preds["type"][count],
                            "mac": info_preds["mac"][count],
                            "architecture": info_preds["architecture"][count],
                            "event": info_preds["event"][count],
                            "executable": info_preds["executable"][count],
                            "md5_hash": info_preds["md5_hash"][count],
                            "sha256_hash": info_preds["sha256_hash"][count],
                            "prediction": str(pred),
                            "confidence_score": str(cs[count]),
                            "model": model_type
                        }
                        # 5. Push to kafka-topic
                        pd_df = pd.DataFrame([result])
                        helpers.bulk(elastic_client, pd_df.transpose().to_dict().values(), index=elastic_index)
                print("Total sysmon records", len(feature_creation_without_info))
                print("Total Preds: ", cnt)
            # reset raw_data bucket
            raw_data = []
